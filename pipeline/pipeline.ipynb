{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Enable interactive plot\n",
    "#@formatter:off\n",
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#@formatter:on\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as ptch\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import cv2\n",
    "from math import ceil\n",
    "from scipy.signal import find_peaks, butter, filtfilt\n",
    "from scipy.stats import linregress\n",
    "from sklearn import linear_model\n",
    "from IPython.display import display_html \n",
    "\n",
    "from metrics.Maokai import mpbrpe, mpkpe, pck, pmpkpe, bone_lengths, distance_ordinal_histogram\n",
    "from loss.CompositionalLoss import parents\n",
    "\n",
    "from datasets.RealSenseOptitrackDataset import RealSenseOptitrackDataset as Dataset\n",
    "from datasets.RealSenseOptitrackDataset import marker_set\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.LinearRegression.LinearRegression_pytorch as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_default\n",
    "neural_network = NeuralNetwork.LinearRegression(hyper_params['input_size'], hyper_params['output_size'])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters())\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/LinearRegression/LinearRegression_default/2022-07-28_15-07-01\"\n",
    "model_name = 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Regression Model without Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.LinearRegression.LinearRegression_pytorch as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_no_rot\n",
    "neural_network = NeuralNetwork.LinearRegression(hyper_params['input_size'], hyper_params['output_size'])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters())\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/LinearRegression/LinearRegression_no_rot/2022-09-23_16-40-33\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.BaselineNN.BaselineNN as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_default\n",
    "neural_network = NeuralNetwork.BaselineNN(hyper_params['output_size'])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/BaselineNN/BaselineNN_default/2022-09-30_12-51-28\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline NN without Rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.BaselineNN.BaselineNN as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_no_rot\n",
    "neural_network = NeuralNetwork.BaselineNN(hyper_params['output_size'])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/BaselineNN/BaselineNN_no_rot/2022-09-30_12-39-16\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Own Baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.OwnBaselineCNN.OwnBaselineCNN as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_default\n",
    "neural_network = NeuralNetwork.NeuralNetwork(hyper_params['output_size'])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/OwnBaselineCNN/OwnBaselineCNN_default/2022-09-29_17-51-33\"\n",
    "model_name = \"model_epoch_10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Own Baseline CNN no Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.OwnBaselineCNN.OwnBaselineCNN as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_no_rot\n",
    "neural_network = NeuralNetwork.NeuralNetwork(hyper_params['output_size'])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/OwnBaselineCNN/OwnBaselineCNN_no_rot/2022-09-29_17-44-20\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CNNModel LiChan2014 with AvgPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.CNNModel_LiChan2014_AvgPool.CNNModel_LiChan2014_AvgPool as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_default\n",
    "neural_network = NeuralNetwork.NeuralNetwork(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = 'models/CNNModel_LiChan2014_AvgPool/CNNModel_LiChan2014_AvgPool_default/2022-07-28_17-24-43'\n",
    "model_name = \"model_epoch_25\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CNNModel LiChan2014 with AvgPool no Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.CNNModel_LiChan2014_AvgPool.CNNModel_LiChan2014_AvgPool as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_no_rot\n",
    "neural_network = NeuralNetwork.NeuralNetwork(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = 'models/CNNModel_LiChan2014_AvgPool/CNNModel_LiChan2014_AvgPool_no_rot/2022-10-06_21-13-50'\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CNNModel LiChan2014 with AvgPool no Rotation low LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.CNNModel_LiChan2014_AvgPool.CNNModel_LiChan2014_AvgPool as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_no_rot_low_lr\n",
    "neural_network = NeuralNetwork.NeuralNetwork(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = 'models/CNNModel_LiChan2014_AvgPool/CNNModel_LiChan2014_AvgPool_no_rot_low_lr/2022-10-10_12-04-08'\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CNNModel LiChan2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.CNNModel_LiChan2014.CNNModel_LiChan2014 as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_default\n",
    "neural_network = NeuralNetwork.NeuralNetwork(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/CNNModel_LiChan2014/CNNModel_LiChan2014_default/2022-07-29_11-20-17\"\n",
    "model_name = \"model_epoch_10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CNNModel LiChan2014 no Rotation with Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.CNNModel_LiChan2014.CNNModel_LiChan2014 as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_no_rot_shuffle\n",
    "neural_network = NeuralNetwork.NeuralNetwork(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/CNNModel_LiChan2014/CNNModel_LiChan2014_no_rotation_shuffle/2022-08-02_12-47-35\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ResNet50 SunShangetAl with L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.ResNet50_SunShangetAl.ResNet50_SunShangetAl as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_l1_loss\n",
    "neural_network = NeuralNetwork.create_nn(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"], \n",
    "                                      momentum=hyper_params[\"momentum\"], weight_decay=hyper_params[\"weight_decay\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/ResNet50_SunShangetAl/ResNet50_SunShangetAl_l1_loss/2022-10-24_16-55-31\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ResNet50 SunShangetAl with L1 Loss no Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.ResNet50_SunShangetAl.ResNet50_SunShangetAl as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_l1_loss_no_rot\n",
    "neural_network = NeuralNetwork.create_nn(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"], )\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/ResNet50_SunShangetAl/ResNet50_SunShangetAl_l1_loss_no_rot/2022-10-06_21-43-33\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ResNet50 SunShangetAl with MSE Loss no Rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.ResNet50_SunShangetAl.ResNet50_SunShangetAl as NeuralNetwork\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_mse_loss_no_rot\n",
    "neural_network = NeuralNetwork.create_nn(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params[\"loss_function\"]\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/ResNet50_SunShangetAl/ResNet50_SunShangetAl_mse_loss_no_rot/2022-10-06_21-41-15\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ResNet50 SunShangetAl with Compositional Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import models.ResNet50_SunShangetAl.ResNet50_SunShangetAl as NeuralNetwork\n",
    "import loss.CompositionalLoss as compositional_loss\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_compositional_loss\n",
    "neural_network = NeuralNetwork.create_nn(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params['loss_function'].CompositionalLoss()\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"], \n",
    "                                      momentum=hyper_params[\"momentum\"], weight_decay=hyper_params[\"weight_decay\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"models/ResNet50_SunShangetAl/ResNet50_SunShangetAl_compositional_loss/2022-10-21_14-16-52\"\n",
    "model_name = \"model_epoch_5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ResNet50 SunShangetAl with Compositional Loss and custom LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import models.ResNet50_SunShangetAl.ResNet50_SunShangetAl as NeuralNetwork\n",
    "import loss.CompositionalLoss as compositional_loss\n",
    "\n",
    "hyper_params = NeuralNetwork.hyper_params_compositional_loss_custom_lr\n",
    "neural_network = NeuralNetwork.create_nn(hyper_params)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = hyper_params['loss_function'].CompositionalLoss()\n",
    "optimizer = hyper_params[\"optimizer\"](neural_network.parameters(), lr=hyper_params[\"learning_rate\"])\n",
    "\n",
    "#Path for loading trained model\n",
    "output_path = \"\"\n",
    "model_name = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(neural_network, (1, 480, 848))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop_rotation = not hyper_params[\"use_rotation_data\"]\n",
    "normalize = hyper_params[\"normalize\"]\n",
    "\n",
    "print(\"\\rLoading data... (1/6)\", end='')\n",
    "session_1_data = Dataset(\n",
    "    csv_file=f'../recordings/2022-04-20/session_1/optitrack/session_1.csv',\n",
    "    root_dir='../recordings/2022-04-20/session_1/realsense/npy',\n",
    "    drop_rotation=drop_rotation,\n",
    "    normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"\\rLoading data... (2/6)\", end='')\n",
    "session_2_data = Dataset(\n",
    "    csv_file=f'../recordings/2022-04-20/session_2/optitrack/session_2.csv',\n",
    "    root_dir='../recordings/2022-04-20/session_2/realsense/npy',\n",
    "    drop_rotation=drop_rotation,\n",
    "    normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"\\rLoading data... (3/6)\", end='')\n",
    "session_3_data = Dataset(\n",
    "    csv_file=f'../recordings/2022-04-20/session_3/optitrack/session_3.csv',\n",
    "    root_dir='../recordings/2022-04-20/session_3/realsense/npy',\n",
    "    drop_rotation=drop_rotation,\n",
    "    normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"\\rLoading data... (4/6)\", end='')\n",
    "session_4_data = Dataset(\n",
    "    csv_file=f'../recordings/2022-04-20/session_4/optitrack/session_4.csv',\n",
    "    root_dir='../recordings/2022-04-20/session_4/realsense/npy',\n",
    "    drop_rotation=drop_rotation,\n",
    "    normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"\\rLoading data... (5/6)\", end='')\n",
    "session_5_data = Dataset(\n",
    "    csv_file=f'../recordings/2022-04-20/session_5/optitrack/session_5.csv',\n",
    "    root_dir='../recordings/2022-04-20/session_5/realsense/npy',\n",
    "    drop_rotation=drop_rotation,\n",
    "    normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"\\rLoading data... (6/6)\", end='')\n",
    "session_6_data = Dataset(\n",
    "    csv_file=f'../recordings/2022-04-20/session_6/optitrack/session_6.csv',\n",
    "    root_dir='../recordings/2022-04-20/session_6/realsense/npy',\n",
    "    drop_rotation=drop_rotation,\n",
    "    normalize=normalize\n",
    ")\n",
    "\n",
    "print(\"\\rLoading data... Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = hyper_params[\"batch_size\"]\n",
    "NUM_WORKERS = 20\n",
    "print(\"Composing Datasets... \", end='')\n",
    "\n",
    "train_data = torch.utils.data.ConcatDataset([session_1_data, session_2_data, session_3_data, session_4_data, session_5_data])\n",
    "test_data = session_6_data\n",
    "all_data = torch.utils.data.ConcatDataset([session_1_data, session_2_data, session_3_data, session_4_data, session_5_data, session_6_data])\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=hyper_params[\"shuffle\"],\n",
    "                                               num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "all_dataloader = torch.utils.data.DataLoader(all_data, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "all_dataloader_no_batch = torch.utils.data.DataLoader(all_data, batch_size=1, shuffle=False,\n",
    "                                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "session_1_dataloader = torch.utils.data.DataLoader(session_1_data, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                              num_workers=NUM_WORKERS, pin_memory=True)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clear gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Move to GPU\n",
    "neural_network.cuda()\n",
    "\n",
    "# track statistics\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# storage and logging stuff\n",
    "start_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "output_path = f\"models/{NeuralNetwork.name}/{hyper_params['name']}/{start_time}\"\n",
    "os.makedirs(output_path)\n",
    "\n",
    "# logging\n",
    "if log.hasHandlers():\n",
    "    log.handlers.clear()\n",
    "start_time = datetime.now().strftime('%Y_%m_%d_%H.%M.%S')\n",
    "log.addHandler(logging.FileHandler(f\"{output_path}/training.log\"))\n",
    "log.warning(hyper_params)\n",
    "log.addHandler(logging.StreamHandler(sys.stdout))\n",
    "log.setLevel('WARN')\n",
    "log.warning(f\"\\nStart training of model: {hyper_params['name']}\")\n",
    "\n",
    "# Write loss to file\n",
    "file = open(f\"{output_path}/loss.csv\", \"w+\", buffering=1)\n",
    "file.write(f\",{str(hyper_params['loss_function'])},{str(hyper_params['loss_function'])}\\n\")\n",
    "file.write(\"epoch,train_loss,val_loss\\n\")\n",
    "\n",
    "# Learning rate decay\n",
    "scheduler = MultiStepLR(optimizer, milestones=hyper_params['lr_decay_milestones'], gamma=hyper_params['lr_decay_gamma'])\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(1, hyper_params[\"epochs\"] + 1):\n",
    "    \n",
    "    # Print epoch\n",
    "    log.warning(f'\\nStarting epoch {epoch}/{hyper_params[\"epochs\"]}')\n",
    "    file.write(f\"{epoch},\")\n",
    "\n",
    "    # Set current loss value\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Training\n",
    "    neural_network.train()\n",
    "    for i, data in enumerate(train_dataloader, 1):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs = data['realsense']\n",
    "        targets = data['optitrack']\n",
    "        inputs, targets = inputs.float().cuda(), targets.float().cuda()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = neural_network(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "#         print(f'loss: {loss.item()}')\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        total_train_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            log.warning(f'    Train loss after mini-batch {i:5d}: {total_train_loss / i:.3f}')\n",
    "    total_train_loss = total_train_loss / i\n",
    "    train_loss.append(total_train_loss)\n",
    "    file.write(f\"{total_train_loss},\")\n",
    "\n",
    "    # Validation\n",
    "    neural_network.eval()\n",
    "    for i, data in enumerate(test_dataloader, 1):\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Get and prepare inputs\n",
    "                inputs = data['realsense']\n",
    "                targets = data['optitrack']\n",
    "                inputs, targets = inputs.float().cuda(), targets.float().cuda()\n",
    "\n",
    "                # Perform forward pass\n",
    "                outputs = neural_network(inputs)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = loss_function(outputs, targets)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "    \n",
    "    total_val_loss = total_val_loss / i\n",
    "    val_loss.append(total_val_loss)\n",
    "    file.write(f\"{total_val_loss}\\n\")\n",
    "\n",
    "    log.warning(f\"Epoch: {epoch}/{hyper_params['epochs']}, Train Loss: {total_train_loss:.8f}, Val Loss: {total_val_loss:.8f}\")\n",
    "\n",
    "    # Save intermediate model\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(neural_network.state_dict(), f\"{output_path}/model_epoch_{epoch}.pth\")\n",
    "        log.warning(f'Model {output_path}/model_epoch_{epoch}.pth saved!')\n",
    "    \n",
    "    #LR decay\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "# Process is complete.\n",
    "log.warning('Training process has finished.')\n",
    "file.close()\n",
    "\n",
    "# Save model\n",
    "log.warning('Saving model... ')\n",
    "torch.save(neural_network.state_dict(), f\"{output_path}/model.pth\")\n",
    "log.warning(f\"Saved: {output_path}/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.warning('Saving model... ')\n",
    "torch.save(neural_network.state_dict(), f\"{output_path}/model.pth\")\n",
    "log.warning(f\"Saved: {output_path}/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"From: {output_path}/{model_name}.pth\")\n",
    "print('Loading model... ', end='')\n",
    "neural_network.cuda()\n",
    "neural_network.load_state_dict(torch.load(f\"{output_path}/{model_name}.pth\"))\n",
    "print(f\"{hyper_params['name']} loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Show training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_loss_csv(output_path: str, train_name=\"Trainingsfehler\", val_name=\"Validierungsfehler\", \n",
    "                  val_trend=True, normalize=False):\n",
    "    loss_df = pd.read_csv(f\"{output_path}/loss.csv\", skiprows=1, index_col='epoch')\n",
    "    loss_df.columns = [train_name, val_name]\n",
    "    loss_df.index.name = 'Epoche'\n",
    "    if normalize:\n",
    "        min_df = loss_df.min().min()\n",
    "        max_df = loss_df.max().max()\n",
    "        loss_df = (loss_df - min_df) / (max_df - min_df)\n",
    "#         loss_df= (loss_df - loss_df.mean()) / loss_df.std()\n",
    "    if val_trend:\n",
    "        epochs = loss_df.index.size\n",
    "        regr = linear_model.LinearRegression()\n",
    "        x = np.arange(1, epochs+1).reshape(epochs,1)\n",
    "        y = loss_df[\"Validierungsfehler\"].values.reshape(epochs,1)\n",
    "        regr.fit(x, y)\n",
    "        predict = regr.predict(x)\n",
    "        loss_df['Trend des Validierungsfehlers'] = predict\n",
    "    return loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(read_loss_csv(output_path, normalize=False, val_trend=False) / 63).plot(\n",
    "    ylabel='Compositional Loss\\npro Schlüsselpunkt',\n",
    "    title='Trainings- und Validierungsfehler\\nModell 6d)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (read_loss_csv(\"models/CNNModel_LiChan2014_AvgPool/CNNModel_LiChan2014_AvgPool_default/2022-07-28_17-24-43\", val_trend=False, \n",
    "                       train_name=\"5a) Trainingsfehler\", \n",
    "                       val_name=\"5a) Validierungsfehler\") / 126).plot()\n",
    "ax = (read_loss_csv(\"models/CNNModel_LiChan2014_AvgPool/CNNModel_LiChan2014_AvgPool_no_rot/2022-10-06_21-13-50\", val_trend=False, \n",
    "                       train_name=\"5b) Trainingsfehler\", \n",
    "                       val_name=\"5b) Validierungsfehler\") / 63).plot(ax=ax, linestyle='dotted')\n",
    "ax = (read_loss_csv(\"models/CNNModel_LiChan2014_AvgPool/CNNModel_LiChan2014_AvgPool_no_rot_low_lr/2022-10-10_12-04-08\", val_trend=False, \n",
    "                          train_name=\"5c) Trainingsfehler\",\n",
    "                          val_name=\"5c) Validierungsfehler\") / 63).plot(ax=ax, linestyle='dashdot',\n",
    "                                                                title='Trainings- und Validierungsfehler',\n",
    "                                                                xlabel='Epochen', \n",
    "                                                                ylabel='Mittlere Quadratische Abweichung \\npro Schlüsselpunkt')\n",
    "\n",
    "\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3,ncols=1,figsize=(9,14))\n",
    "mse_no_rot[0:25].plot(ax = axes[0], title=\"Mittlere Quadratische Abweichung\")\n",
    "l1_no_rot[0:25].plot(ax = axes[1])\n",
    "l1_joints.plot(ax = axes[1], title=\"L1-Norm\")\n",
    "comp_loss.plot(ax = axes[2], title=\"Compositional Loss\")\n",
    "plt.legend(loc='center right')\n",
    "axes[0].legend(loc='center right')\n",
    "fig.tight_layout(pad=2.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelvis_prediction = np.load('models/CNN_Pelvis_Location/CNN_Pelvis_Location_default/2022-10-19_12-51-20/prediction.npy')\n",
    "pelvis_prediction_tiled = np.tile(pelvis_prediction, 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = []\n",
    "neural_network.eval()\n",
    "total_mpkpe = 0\n",
    "total_pmpkpe = 0\n",
    "total_mpbrpe = 0\n",
    "total_pck = 0\n",
    "total_bone_lengths = None\n",
    "total_bone_lengths_gt = None\n",
    "\n",
    "dataloader = test_dataloader\n",
    "\n",
    "compute_metrics = False\n",
    "\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Get and prepare inputs\n",
    "            inputs = data['realsense']\n",
    "            targets = data['optitrack']\n",
    "            inputs, targets = inputs.float().cuda(), targets.float().cuda()\n",
    "\n",
    "            # Perform forward pass\n",
    "            outputs = neural_network(inputs)\n",
    "            \n",
    "            if hyper_params[\"normalize\"]:\n",
    "                outputs = test_data.denormalize_cuda(outputs)\n",
    "                targets = test_data.denormalize_cuda(targets)\n",
    "            \n",
    "            # outputs of net are relative to parent bone/joint\n",
    "            # compose skeleton\n",
    "#             outputs = compose_output(outputs)\n",
    "            # place outputs in correct position in space\n",
    "#             outputs += pelvis_prediction_tiled_tensor[i * dataloader.batch_size : (i + 1) * dataloader.batch_size]\n",
    "            \n",
    "            prediction.append(outputs.cpu().numpy())\n",
    "            \n",
    "            # Compute evaluation metrics\n",
    "            if compute_metrics:\n",
    "                total_pck += pck(outputs, targets)\n",
    "                total_pmpkpe += pmpkpe(outputs, targets, has_rot_data=hyper_params[\"use_rotation_data\"])\n",
    "                total_bone_lengths = bone_lengths(outputs, hyper_params[\"use_rotation_data\"],\n",
    "                                                  total_bone_lengths, 'joints')\n",
    "\n",
    "#                 total_bone_lengths = bone_lengths(outputs, total_bone_lengths, 'bones')\n",
    "\n",
    "                if hyper_params[\"use_rotation_data\"]:\n",
    "                    total_mpbrpe += mpbrpe(outputs, targets)\n",
    "                else:\n",
    "                    total_mpkpe += mpkpe(outputs, targets)\n",
    "\n",
    "            # Show progress\n",
    "            print(f\"\\rProgress: {i/len(dataloader)*100:.1f}%   \", end='')\n",
    "\n",
    "prediction = np.concatenate(prediction)\n",
    "print(\"\\rProgress: Done!  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if hyper_params[\"use_rotation_data\"]: \n",
    "    print(f\"MPKPE: {np.round(total_mpbrpe[0].cpu().numpy() / i, 2):.02f}\")\n",
    "    print(f\"MPBRE: {np.round(total_mpbrpe[1].cpu().numpy() / i, 2):.02f}\")\n",
    "else:\n",
    "    print(f\"MPKPE: {np.round(total_mpkpe.cpu().numpy() / i, 2):.02f}\")\n",
    "\n",
    "print(f\"PCK: {np.round(total_pck.cpu().numpy() / i, 2):.02f}\")\n",
    "print(f\"PMPKPE: {np.round(total_pmpkpe.cpu().numpy() / i, 2):.02f}\")\n",
    "    \n",
    "\n",
    "# print('\\nPrediction:')\n",
    "std_bones = []\n",
    "for bone_name, bone_std in zip(parents.keys(), total_bone_lengths):\n",
    "    std_bones.append(np.std(bone_std))\n",
    "#     print(f\"  {bone_name} Std.: {np.std(bone_std)}\")\n",
    "print(f'MBS: {np.round(np.mean(std_bones), 2):.02f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Ground truth Bone Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_bone_lengths_gt = None\n",
    "for i, data in enumerate(all_dataloader, 0):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Get and prepare inputs\n",
    "            targets = data['optitrack'].float().cuda()\n",
    "            \n",
    "            # Compute bone stability\n",
    "            total_bone_lengths_gt = bone_lengths(targets, hyper_params[\"use_rotation_data\"],\n",
    "                                                 total_bone_lengths_gt, 'joints')\n",
    "            \n",
    "            # Show progress\n",
    "            if i % 50 == 0 :\n",
    "                print(f\"\\rProgress: {i/len(all_data)*100*all_dataloader.batch_size:.1f}%   \", end='')\n",
    "print(\"\\rProgress: Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For All Bones:')\n",
    "std_bones_gt = []\n",
    "for bone_name, bone_std in zip(parents.keys(), total_bone_lengths_gt): \n",
    "    std_bones_gt.append(np.std(bone_std))\n",
    "    print(f\"  {bone_name} Std.: {np.round(np.std(bone_std), 2):.2f}\")\n",
    "print(f'  Mean Std.: {np.round(np.mean(std_bones_gt), 2):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For Lower Bones only:')\n",
    "std_bones_gt = []\n",
    "for bone_name, bone_std in zip(parents.keys(), total_bone_lengths_gt): \n",
    "    if bone_name not in ['LFoot', 'LToe', 'RFoot', 'RToe', 'LShin', 'LThigh', 'RShin', 'RThigh', 'Hip']:\n",
    "        continue\n",
    "    std_bones_gt.append(np.std(bone_std))\n",
    "    print(f\"  {bone_name} Std.: {np.round(np.std(bone_std), 2):.2f}\")\n",
    "print(f'  Mean Std.: {np.round(np.mean(std_bones_gt), 2):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Realsense background Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_patches = 10\n",
    "patch_size = 5\n",
    "patch_long = 550\n",
    "\n",
    "step_size = ceil(480/(number_patches+1))\n",
    "patch_coordinates = list(range(step_size, 480, step_size))\n",
    "patches = np.ndarray((len(all_dataloader_no_batch), number_patches))\n",
    "holes_percentages = np.ndarray(len(all_dataloader_no_batch))\n",
    "\n",
    "for i, data in enumerate(all_dataloader_no_batch, 0):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Get and prepare inputs\n",
    "            inputs = data['realsense'].numpy().squeeze(axis=1).squeeze(axis=0)\n",
    "                                    \n",
    "            # Compute patches   \n",
    "            patches[i] = [inputs[x:x+patch_size, patch_long:patch_long+patch_size].mean() for x in patch_coordinates]\n",
    "\n",
    "            # Compute Holes percentage\n",
    "            holes_percentages[i] = (inputs.size - np.count_nonzero(inputs)) / inputs.size * 100\n",
    "                              \n",
    "            # Show depth image\n",
    "#             if i == 440:\n",
    "# #             if i in excerpts:\n",
    "#                 depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(inputs, alpha=0.03), cv2.COLORMAP_JET)\n",
    "#                 fig, ax = plt.subplots()\n",
    "#                 ax.imshow(depth_colormap)\n",
    "#                 rects = [ptch.Rectangle((patch_long, x), patch_size, patch_size, linewidth=1, edgecolor='none', facecolor='black', label=f'Patch: {i}') for i, x in enumerate(patch_coordinates)]\n",
    "#                 for rect in rects:\n",
    "#                     ax.add_patch(rect)\n",
    "#                 plt.show()\n",
    "# #                 plt.savefig(f'matplotvideo/frame_{i:05d}.png')\n",
    "#                 break\n",
    "\n",
    "            \n",
    "            # Show progress\n",
    "            if i % 50 == 0 :\n",
    "                print(f\"\\rProgress: {i/len(all_data)*100*all_dataloader_no_batch.batch_size:.1f}%   \", end='')\n",
    "print(\"\\rProgress: Done!  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(holes_percentages.std(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_testperson_frames = list(range(440,680)) + list(range(900, 1140)) + list(range(40170, 40370)) + list(range(41100, 41300))\\\n",
    "+ list(range(100550, 100770)) + list(range(101745, 101950))  + list(range(151580, 152000))\n",
    "\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 0])\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 1])\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 2])\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 3])\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 4])\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 5])\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 6])\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 7])\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 8])\n",
    "plt.plot(no_testperson_frames, patches[no_testperson_frames, 9])\n",
    "\n",
    "start=150000\n",
    "end=start+3000\n",
    "x = list(range(start, end))\n",
    "# plt.plot(x, patches[start:end, 0])\n",
    "# plt.plot(x, patches[start:end, 1])\n",
    "# plt.plot(x, patches[start:end, 2])\n",
    "# plt.plot(x, patches[start:end, 3])\n",
    "# plt.plot(x, patches[start:end, 4])\n",
    "# plt.plot(x, patches[start:end, 5])\n",
    "# plt.plot(x, patches[start:end, 6])\n",
    "# plt.plot(x, patches[start:end, 7])\n",
    "# plt.plot(x, patches[start:end, 8])\n",
    "# plt.plot(x, patches[start:end, 9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_avg_distances = patches[no_testperson_frames].mean(axis=0)\n",
    "patches_std = patches[no_testperson_frames].std(axis=0)\n",
    "\n",
    "plt.scatter(patches_avg_distances, patches_std)\n",
    "for i in range(number_patches):\n",
    "    plt.annotate(i, (patches_avg_distances[i], patches_std[i]))\n",
    "\n",
    "plt.ylabel('Standardabweichung')\n",
    "plt.xlabel('Entfernung in mm')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualize as Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_frame = 0\n",
    "FPS = 15\n",
    "length = FPS*105\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 90\n",
    "plt.rcParams['savefig.dpi'] = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SetUp 3D Plots\n",
    "def setup_3d_scatter(fig: plt.Figure, position: int, data: pd.DataFrame, title_prefix='3D Test'):\n",
    "    ax = fig.add_subplot(position, projection='3d')\n",
    "    ax.set_xlabel('X in cm')\n",
    "    ax.set_ylabel('Y in cm')\n",
    "    ax.set_zlabel('Z in cm')\n",
    "\n",
    "    all_x, all_y, all_z = get_bone_coordinates(data)\n",
    "    all_x = all_x[np.logical_not(np.isnan(all_x))]\n",
    "    all_y = all_y[np.logical_not(np.isnan(all_y))]\n",
    "    all_z = all_z[np.logical_not(np.isnan(all_z))]\n",
    "\n",
    "    ax.set_xlim3d(all_x.min() - 10, all_x.max() + 10)\n",
    "    ax.set_ylim3d(all_y.min() - 10, all_y.max() + 10)\n",
    "    ax.set_zlim3d(all_z.min() - 10, all_z.max() + 10)\n",
    "\n",
    "    ax.set_xlim3d(-200, 200)\n",
    "    ax.set_ylim3d(-200, 200)\n",
    "    ax.set_zlim3d(0, 200)\n",
    "\n",
    "#     title = ax.set_title(title_prefix)\n",
    "    title = None\n",
    "\n",
    "    xs, ys, zs = get_bone_coordinates(data.iloc[[0]])\n",
    "    graph = ax.scatter(xs, ys, zs, c='tab:blue', label=\"Model 6b)\")\n",
    "\n",
    "    length = len(all_x)\n",
    "\n",
    "    return graph, data, title, length, title_prefix\n",
    "\n",
    "\n",
    "def get_bone_coordinates(df: pd.DataFrame):\n",
    "    xs = np.concatenate([df[x].Position.Z.values for x in marker_set])\n",
    "    ys = np.concatenate([df[y].Position.X.values for y in marker_set])\n",
    "    zs = np.concatenate([df[z].Position.Y.values for z in marker_set])\n",
    "    return xs, ys, zs\n",
    "\n",
    "\n",
    "def update(num):\n",
    "#     update_graph(num, data_left, graph_left, title_left, title_prefix_left)\n",
    "    update_graph(num, data_right, graph_right, title_right, title_prefix_right)\n",
    "\n",
    "\n",
    "def update_graph(num, data, graph, title, title_prefix: str):\n",
    "    num += start_frame\n",
    "    data = data.iloc[[num]]\n",
    "    graph._offsets3d = get_bone_coordinates(data)\n",
    "#     title.set_text(f'{title_prefix}, time={num}')\n",
    "\n",
    "# Prepare data\n",
    "left_graph_data = session_1_data.optitrack_data\n",
    "if hyper_params[\"normalize\"]:\n",
    "    left_graph_data = test_data.denormalize(left_graph_data)\n",
    "\n",
    "right_graph_data = pd.DataFrame(prediction)\n",
    "right_graph_data.columns = test_data.optitrack_data.columns\n",
    "\n",
    "\n",
    "# draw 3D animation of model\n",
    "fig = plt.figure()\n",
    "\n",
    "# graph_left, data_left, title_left, length, title_prefix_left = setup_3d_scatter(fig, 121,\n",
    "#                                                                                 left_graph_data, \"Grundwahrheit\")\n",
    "graph_right, data_right, title_right, _, title_prefix_right = setup_3d_scatter(fig, 111, \n",
    "                                                                               right_graph_data, \"Vorhersage\")\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "anim = matplotlib.animation.FuncAnimation(fig, update, length, interval=1000/FPS, blit=False)\n",
    "\n",
    "\n",
    "\n",
    "f = f\"./model6b-pred_{start_frame}-{start_frame+length}_{FPS}fps.mp4\" \n",
    "writervideo = matplotlib.animation.FFMpegWriter(fps=FPS) \n",
    "anim.save(f, writer=writervideo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize in one Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SetUp 3D Plots\n",
    "def get_limit(data: pd.DataFrame):\n",
    "    all_x, all_y, all_z = get_bone_coordinates(data)\n",
    "    all_x = all_x[np.logical_not(np.isnan(all_x))]\n",
    "    all_y = all_y[np.logical_not(np.isnan(all_y))]\n",
    "    all_z = all_z[np.logical_not(np.isnan(all_z))]\n",
    "    return all_x.min(), all_x.max(), all_y.min(), all_y.max(), all_z.min(), all_z.max()\n",
    "\n",
    "def get_limits(data1: pd.DataFrame, data2: pd.DataFrame):\n",
    "    xmin1, xmax1, ymin1, ymax1, zmin1, zmax1 = get_limit(data1)\n",
    "    xmin2, xmax2, ymin2, ymax2, zmin2, zmax2 = get_limit(data2)\n",
    "    return min(xmin1, xmin2), max(xmax1, xmax2), min(ymin1, ymin2), max(ymax1, ymax2), min(zmin1, zmin2), max(zmax1, zmax2) \n",
    "\n",
    "def setup_3d_scatter(fig: plt.Figure, data1: pd.DataFrame, data2: pd.DataFrame,\n",
    "                     title_prefix: str):\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_xlabel('X in cm')\n",
    "    ax.set_ylabel('Y in cm')\n",
    "    ax.set_zlabel('Z in cm')\n",
    "\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = get_limits(data1, data2)  \n",
    "    ax.set_xlim3d(xmin - 10, xmax + 10)\n",
    "    ax.set_ylim3d(ymin - 10, ymax + 10)\n",
    "    ax.set_zlim3d(zmin - 10, zmax + 10)\n",
    "\n",
    "    ax.set_xlim3d(-200, 200)\n",
    "    ax.set_ylim3d(-200, 200)\n",
    "    ax.set_zlim3d(0, 200)\n",
    "\n",
    "    if title_prefix:\n",
    "        title = ax.set_title(title_prefix)\n",
    "\n",
    "    xs1, ys1, zs1 = get_bone_coordinates(data1.iloc[[0]])\n",
    "    xs2, ys2, zs2 = get_bone_coordinates(data2.iloc[[0]])\n",
    "    graph1 = ax.scatter(xs1, ys1, zs1, c='tab:orange', label='Grundwahrheit')\n",
    "    graph2 = ax.scatter(xs2, ys2, zs2, c='tab:blue', label='Model 6b)')\n",
    "    \n",
    "    return graph1, graph2\n",
    "\n",
    "def get_bone_coordinates(df: pd.DataFrame):\n",
    "    xs = np.concatenate([df[x].Position.Z.values for x in marker_set])\n",
    "    ys = np.concatenate([df[y].Position.X.values for y in marker_set])\n",
    "    zs = np.concatenate([df[z].Position.Y.values for z in marker_set])\n",
    "    return xs, ys, zs\n",
    "\n",
    "def update(num):\n",
    "    num += start_frame\n",
    "    update_graph(num, red_data, graph1)\n",
    "    update_graph(num, blue_data, graph2)\n",
    "\n",
    "\n",
    "def update_graph(num, data, graph):\n",
    "    data = data.iloc[[num]]\n",
    "    graph._offsets3d = get_bone_coordinates(data)\n",
    "    \n",
    "# Prepare data\n",
    "red_data = test_data.optitrack_data\n",
    "if hyper_params[\"normalize\"]:\n",
    "    red_data = test_data.denormalize(red_data)\n",
    "\n",
    "blue_data = pd.DataFrame(prediction)\n",
    "blue_data.columns = test_data.optitrack_data.columns\n",
    "# blue_data += np.tile(pelvis_prediction, 21)\n",
    "\n",
    "# draw 3D animation of model\n",
    "fig = plt.figure()\n",
    "\n",
    "graph1, graph2 = setup_3d_scatter(fig, red_data, blue_data, None)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# length = len(red_data)\n",
    "\n",
    "anim = matplotlib.animation.FuncAnimation(fig, update, length, interval=1000/FPS, blit=False)\n",
    "\n",
    "# f = f\"./model6b-pred+gt_{start_frame}-{start_frame+length}_{FPS}fps.mp4\" \n",
    "# writervideo = matplotlib.animation.FFMpegWriter(fps=FPS) \n",
    "# anim.save(f, writer=writervideo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Realsense Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis('off')\n",
    "for i in range(start_frame, start_frame+length):\n",
    "    image = test_data[i]['realsense'].numpy().squeeze()\n",
    "    image = np.flipud(image)\n",
    "    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "    \n",
    "    images.append([ax.imshow(depth_colormap,animated=True)])\n",
    "    \n",
    "    # Show progress\n",
    "    print(f\"\\rProgress: {i/length*100:.1f}%   \", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 300 /3\n",
    "plt.rcParams['savefig.dpi'] = 300 /3\n",
    "\n",
    "anim = matplotlib.animation.ArtistAnimation(fig, images, interval=1000/FPS, blit=False)\n",
    "# plt.show()\n",
    "\n",
    "f = f\"./realsense_{start_frame}-{start_frame+length}_{FPS}fps.mp4\" \n",
    "writervideo = matplotlib.animation.FFMpegWriter(fps=FPS) \n",
    "anim.save(f, writer=writervideo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Position of Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.DataFrame(columns=session_2_data.optitrack_data.columns)\n",
    "# mean_df.columns = test_data.optitrack_data.columns\n",
    "mean_df.at[0] = test_data.optitrack_data.mean()\n",
    "mean_df = mean_df.loc[mean_df.index.repeat(test_data.optitrack_data.index.size)]\n",
    "mean_df.index = np.arange(test_data.optitrack_data.index.size)\n",
    "mean_df = mean_df.astype(float)\n",
    "left_graph_data = mean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Stride Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prediction)\n",
    "df.columns = test_data.optitrack_data.columns\n",
    "df_gt = test_data.optitrack_data\n",
    "\n",
    "frm=start_frame\n",
    "to=frm+length*1.2\n",
    "df = df.loc[frm:to]\n",
    "df_gt = df_gt.loc[frm:to]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(prediction)\n",
    "# df.columns = test_data.optitrack_data.columns\n",
    "# df_gt = test_data.optitrack_data\n",
    "\n",
    "# frm=start_frame\n",
    "# to=frm+length\n",
    "# df = df.loc[frm:to]\n",
    "# df_gt = df_gt.loc[frm:to]\n",
    "\n",
    "# Low Pass Butterworth Filter\n",
    "def low_pass_filter(data, cutoff=4, sample_rate=30.0, order=2):\n",
    "    normal_cutoff = cutoff / (0.5 * sample_rate)\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='lowpass', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _add_velocity_and_speed(df: pd.DataFrame):\n",
    "    df = df.sort_index(axis=1)\n",
    "    for joint_name in set(df.columns.get_level_values(0)):  # für alle Gelenke Speed berechnen\n",
    "        velocity = df[joint_name, 'Position'].diff()\n",
    "        velocity = velocity.interpolate(axis=0, limit_direction='both')\n",
    "        speed = np.linalg.norm(velocity[['X', 'Y', 'Z']].values, axis=1, ord=2)\n",
    "        speed = low_pass_filter(speed)\n",
    "\n",
    "        df[joint_name, 'Velocity', 'X'] = velocity['X']\n",
    "        df[joint_name, 'Velocity', 'Y'] = velocity['Y']\n",
    "        df[joint_name, 'Velocity', 'Z'] = velocity['Z']\n",
    "        df[joint_name, 'Speed', 'XYZ'] = speed\n",
    "        df = df.sort_index(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _find_valley_indices(df, distance, height=None, threshold=None, prominence=0.2):\n",
    "    return find_peaks(-(df).to_numpy().flatten(), height, threshold, distance, prominence)[0]\n",
    "\n",
    "\n",
    "def calc_standing_foot_positions(df, distance=20):\n",
    "    df = _add_velocity_and_speed(df)\n",
    "    \n",
    "    results = pd.DataFrame(columns=['i', 'pos', 'vec', 'stride_len'], index=['LFoot', 'LToe', 'RFoot', 'RToe'])\n",
    "    for foot, toe in [('LFoot', 'LToe'), ('RFoot', 'RToe')]:\n",
    "                \n",
    "        valley_indices       = _find_valley_indices(df[foot, 'Speed'], distance=distance)\n",
    "        \n",
    "        results.at[foot, 'i']   = valley_indices\n",
    "        results.at[foot, 'pos'] = df.iloc[valley_indices].loc[:, (foot, 'Position')]\n",
    "        results.at[foot, 'vec'] = df.iloc[valley_indices][foot]['Position'].diff()\n",
    "        results.at[foot, 'stride_len'] = np.linalg.norm(results['vec'][foot][['X', 'Y', 'Z']].values, axis=1, ord=2)[1:]\n",
    "        \n",
    "        results.at[toe, 'i']   = valley_indices\n",
    "        results.at[toe, 'pos'] = df.iloc[valley_indices].loc[:, (toe, 'Position')]\n",
    "        results.at[toe, 'vec'] = df.iloc[valley_indices][toe]['Position'].diff()\n",
    "        results.at[toe, 'stride_len'] = np.linalg.norm(results['vec'][toe][['X', 'Y', 'Z']].values, axis=1, ord=2)[1:]   \n",
    "    return results, df\n",
    "\n",
    "\n",
    "prediction_foots, df_valleys    = calc_standing_foot_positions(df)\n",
    "gt_foots        , df_gt_valleys = calc_standing_foot_positions(df_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Valleys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(df_gt_valleys['LFoot', 'Speed'] * 0.3, '-x', markevery=gt_foots['i']['LFoot'], label=\"Grundwahrheit\")\n",
    "ax.plot(df_valleys['LFoot', 'Speed'] *0.3, '-x', markevery=prediction_foots['i']['LFoot'], label=\"Modell 6b)\")\n",
    "plt.xlabel(\"Datenpunkt\")\n",
    "plt.ylabel(\"Geschwindigkeit in m/s\")\n",
    "plt.title(\"Geschwindigkeit des linken Fußes\\n von Modell 6b) und Grundwahrheit im Verlgleich\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated Valley Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 5) / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_gt = df_gt_valleys['LFoot', 'Speed'] * 0.3\n",
    "speed_pred = df_valleys['LFoot', 'Speed'] *0.3\n",
    "valleys_gt = gt_foots['i']['LFoot']\n",
    "valleys_pred = prediction_foots['i']['LFoot']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "line_gt,   = ax.plot([], [], marker='X', markevery=[], label=\"Grundwahrheit\", c='tab:orange')\n",
    "line_pred, = ax.plot([], [], marker='X', markevery=[], label=\"Modell 6b)\", c='tab:blue')\n",
    "\n",
    "plt.xlabel(\"Datenpunkt\")\n",
    "plt.ylabel(\"Geschwindigkeit in m/s\")\n",
    "plt.title(\"Geschwindigkeit des linken Fußes\")\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "frames_to_show = 60*5\n",
    "right_margin=30\n",
    "\n",
    "ax.set_ylim(-0.3, 4.0)\n",
    "ax.set_xlim(-5, frames_to_show+right_margin)\n",
    "\n",
    "def animate(slice_end):\n",
    "    slice_start = slice_end-frames_to_show\n",
    "    for line, speed, valleys in [(line_gt, speed_gt, valleys_gt), (line_pred, speed_pred, valleys_pred)]:\n",
    "        if (slice_start < 0):\n",
    "            x_data = np.arange(0, slice_end)\n",
    "            y_data = speed.iloc[0:slice_end].to_numpy().squeeze()\n",
    "        else:\n",
    "            x_data = np.arange(slice_start, slice_end)\n",
    "            y_data = speed.iloc[slice_start:slice_end].to_numpy().squeeze()\n",
    "            ax.set_xlim(slice_start, slice_end+right_margin)\n",
    "\n",
    "        marker_in_slice = [x - slice_end for x in valleys if x in x_data]   \n",
    "        line.set_data(x_data, y_data)\n",
    "        line.set_markevery(marker_in_slice)\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=length, interval=100/FPS, blit=False)\n",
    "\n",
    "# f = f\"./valley_graph_{start_frame}-{start_frame+length}_{FPS}fps.mp4\" \n",
    "# writervideo = matplotlib.animation.FFMpegWriter(fps=FPS) \n",
    "# anim.save(f, writer=writervideo)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation MPKPE and F-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpkpe =        np.asarray([25.20, 25.03, 28.94, 25.00, 23.61, 19.03, 20.24, 17.62, 21.34, 32.21])\n",
    "f_score =      np.asarray([10.41, 10.63,  7.12, 10.30, 10.64, 14.53, 11.86, 17.71, 14.79,  8.51])\n",
    "step_percent = np.asarray([123.81, 126.15, 106.81, 106.59, 121.89, 121.54, 122.85, 116.04, 120.78, 119.48])\n",
    "\n",
    "x = f_score\n",
    "y = mpkpe\n",
    "# y = step_percent\n",
    "res = linregress(x, y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')\n",
    "plt.xlabel('f-Score')\n",
    "plt.ylabel('MPKPE')\n",
    "plt.show()\n",
    "\n",
    "corr, _ = pearsonr(x, y)\n",
    "print('Pearsons correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stride length distribution as histogram and histogram distance from ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = 80\n",
    "\n",
    "hist1_data = gt_foots['stride_len']['LFoot']\n",
    "hist2_data = prediction_foots['stride_len']['LFoot']\n",
    "\n",
    "# hist_range = (min(hist1_data.min(), hist2_data.min()), max(hist1_data.max(), hist2_data.max()))\n",
    "hist_range = (0, 400)\n",
    "gt_counts, gt_bins = np.histogram(gt_foots['stride_len']['LFoot'], bins=bins, range=hist_range)\n",
    "pred_counts, pred_bins = np.histogram(prediction_foots['stride_len']['LFoot'], bins=bins, range=hist_range)\n",
    "\n",
    "distance = distance_ordinal_histogram(gt_counts, pred_counts, bins)\n",
    "\n",
    "\n",
    "print(f\"Histogramm distance: {distance}\")\n",
    "print(f\"Auflösung Histogramm: {np.round(gt_bins[1]-gt_bins[0], 2)} cm\")\n",
    "\n",
    "plt.figure()\n",
    "plt.stairs(gt_counts, gt_bins, label=\"Grundwahrheit\")\n",
    "plt.stairs(pred_counts, pred_bins, label=\"Modell 5c)\")\n",
    "plt.xlabel(\"Schrittlänge in cm\")\n",
    "plt.ylabel(\"Anzahl der Schritte\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Stride length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for bone in ['LFoot', 'LToe', 'RFoot', 'RToe']:\n",
    "    print(f\"{bone}:\")\n",
    "#     print(f\"  Ground Truth Stride length: \\n{gt_foots.at[bone, 'stride_len']}\")\n",
    "    print(f\"  Predicted Stride length: \\n{prediction_foots.at[bone, 'stride_len']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Stride length Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stride_length_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    metrics = ['step amount', \n",
    "               'stride length mean', \n",
    "               'stride length median', \n",
    "               'stride length std', \n",
    "               'travelled distance'\n",
    "              ]\n",
    "    \n",
    "    df[metrics[0]] = df['stride_len'].apply(len)\n",
    "    df[metrics[1]] = df['stride_len'].apply(np.mean)\n",
    "    df[metrics[2]] = df['stride_len'].apply(np.median)\n",
    "    df[metrics[3]] = df['stride_len'].apply(np.std)\n",
    "    df[metrics[4]] = df['stride_len'].apply(sum)\n",
    "    df.at['__all__'] = df[metrics].mean()\n",
    "    return df, metrics\n",
    "\n",
    "gt_foots, stride_len_metrics = add_stride_length_metrics(gt_foots)\n",
    "prediction_foots, _ = add_stride_length_metrics(prediction_foots)\n",
    "\n",
    "print(\"Ground Truth:\")\n",
    "print(gt_foots[stride_len_metrics].round(3))\n",
    "print(\"\\nPrediction:\")\n",
    "print(prediction_foots[stride_len_metrics].round(3))\n",
    "\n",
    "\n",
    "print(f\"\\nSchrittprozent: {np.round(100 * prediction_foots.at['__all__', 'step amount'] / gt_foots.at['__all__', 'step amount'], 2)}\")\n",
    "print(f\"Mittlere Schrittlängendifferenz: {np.round(prediction_foots.at['__all__', 'stride length mean'] - gt_foots.at['__all__', 'stride length mean'], 2)}\")\n",
    "print(f\"Mittlere Differenz der Schrittlängenstandardabweichung: {np.round(prediction_foots.at['__all__', 'stride length std'] - gt_foots.at['__all__', 'stride length std'], 2)}\")\n",
    "print(f\"Verhältnis Strecke: {np.round(prediction_foots.at['__all__', 'travelled distance'] / gt_foots.at['__all__', 'travelled distance'], 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msld_sampled=np.array([-12.92, -9.64, -3.39, -22.61, -2.8, -0.32, -6.79, -9.13, -1.03, -6.53])\n",
    "np.abs(msld_sampled).mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_confusion_matrix(ground_truth_valleys: np.ndarray, predicted_valleys: np.ndarray, pad_size=3, normalize=True):\n",
    "    \n",
    "    predict = set(predicted_valleys)\n",
    "    true_positive = []\n",
    "    true_negative = [] # list of tuples: [(gt_step_index, prediction_step_index), (...)]\n",
    "    false_positive = []\n",
    "    false_negative = []\n",
    "    confusion_matrix = pd.DataFrame(columns=['step', 'no step', '__all__'],\n",
    "                                      index=['step', 'no step', '__all__'])\n",
    "    confusion_matrix.index.name = 'actual'\n",
    "    confusion_matrix.columns.name = 'predicted'\n",
    "\n",
    "    for last_step_index, cur_step_index in zip(np.insert(ground_truth_valleys, 0, 0), ground_truth_valleys):\n",
    "        pad_start = cur_step_index - pad_size\n",
    "        pad_end = cur_step_index + pad_size\n",
    "        step_pad = set(range(pad_start, pad_end))\n",
    "\n",
    "        # false negative or true positive (and false positive)\n",
    "        steps = predict.intersection(step_pad)\n",
    "        if steps == set():\n",
    "            false_negative.append(cur_step_index)\n",
    "        elif len(steps) == 1:\n",
    "            step, = steps\n",
    "            true_positive.append((cur_step_index, step))\n",
    "        else:\n",
    "            step_list = list(steps)\n",
    "            true_positive.append((cur_step_index, step_list[0]))\n",
    "            false_positive += step_list[1:]\n",
    "\n",
    "        # true negative or false positive\n",
    "        if last_step_index != 0:\n",
    "            last_step_index += pad_size\n",
    "        no_step = set(range(last_step_index, pad_start))\n",
    "        if predict.isdisjoint(no_step):\n",
    "            true_negative.append(no_step)\n",
    "        else:\n",
    "            false_positive += list(predict.intersection(no_step))\n",
    "    \n",
    "   #confusion_matrix.at['actual', 'predcition']\n",
    "    confusion_matrix.at[   'step',    'step'] = len(true_positive)\n",
    "    confusion_matrix.at[   'step', 'no step'] = len(false_negative)\n",
    "    confusion_matrix.at['no step',    'step'] = len(false_positive)\n",
    "    confusion_matrix.at['no step', 'no step'] = len(true_negative)\n",
    "    confusion_matrix.at['__all__'] = confusion_matrix.sum(0)\n",
    "    confusion_matrix['__all__'] = confusion_matrix.sum(1)\n",
    "    confusion_matrix = confusion_matrix.astype(int)\n",
    "    if normalize:\n",
    "        confusion_matrix /= confusion_matrix.at['__all__','__all__']\n",
    "    return confusion_matrix\n",
    "        \n",
    "pad_size=3\n",
    "confusion_matrix_lfoot = calc_confusion_matrix(gt_foots['i']['LFoot'], prediction_foots['i']['LFoot'], pad_size)\n",
    "confusion_matrix_rfoot = calc_confusion_matrix(gt_foots['i']['RFoot'], prediction_foots['i']['RFoot'], pad_size)\n",
    "confusion_matrix_ltoe = calc_confusion_matrix(gt_foots['i']['LToe'], prediction_foots['i']['LToe'], pad_size)\n",
    "confusion_matrix_rtoe = calc_confusion_matrix(gt_foots['i']['RToe'], prediction_foots['i']['RToe'], pad_size)\n",
    "\n",
    "confusion_matrix_mean = (confusion_matrix_lfoot + confusion_matrix_rfoot + \n",
    "                         confusion_matrix_ltoe + confusion_matrix_rtoe) / 4\n",
    "\n",
    "df1_styler = confusion_matrix_lfoot.style.set_table_attributes(\"style='display:inline'\").set_caption('Left Foot')\n",
    "df2_styler = confusion_matrix_rfoot.style.set_table_attributes(\"style='display:inline'\").set_caption('Right Foot')\n",
    "df3_styler = confusion_matrix_ltoe.style.set_table_attributes(\"style='display:inline'\").set_caption('Left Toe')\n",
    "df4_styler = confusion_matrix_rtoe.style.set_table_attributes(\"style='display:inline'\").set_caption('Right Toe')\n",
    "\n",
    "df5_styler = confusion_matrix_mean.style.set_table_attributes(\"style='display:inline'\").set_caption('Mean')\n",
    "\n",
    "space = \"\\xa0\" * 10\n",
    "display_html(df3_styler._repr_html_() + space + df4_styler._repr_html_(), raw=True)\n",
    "display_html(df1_styler._repr_html_() + space + df2_styler._repr_html_(), raw=True)\n",
    "\n",
    "display_html(df5_styler._repr_html_(), raw=True)\n",
    "\n",
    "TP = confusion_matrix_mean.at['step', 'step']\n",
    "FP = confusion_matrix_mean.at['no step', 'step']\n",
    "TN = confusion_matrix_mean.at['no step', 'no step']\n",
    "FN = confusion_matrix_mean.at['step', 'no step']\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = (TP) / (TP + FP)\n",
    "recall = (TP) / (TP + FN)\n",
    "f_score = (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy in %:  {np.round(accuracy*100, 2)}\")\n",
    "print(f\"Precision in %: {np.round(precision*100, 2)}\")\n",
    "print(f\"Recall in %:    {np.round(recall*100, 2)}\")\n",
    "print(f\"F-Score in %:   {np.round(f_score*100, 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Confusion Matrix with Random Foot Indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "total_accuracy  = [] \n",
    "total_precision = []\n",
    "total_recall    = []\n",
    "total_f_score   = []\n",
    "for _ in range(sample_size):\n",
    "    random_foots = np.sort(np.random.randint(low=1, high=len(df_gt), size=int(gt_foots['step amount']['__all__'])))\n",
    "    confusion_matrix_random = calc_confusion_matrix(gt_foots['i']['LFoot'], random_foots)\n",
    "\n",
    "    TP = confusion_matrix_random.at['step', 'step']\n",
    "    FP = confusion_matrix_random.at['no step', 'step']\n",
    "    TN = confusion_matrix_random.at['no step', 'no step']\n",
    "    FN = confusion_matrix_random.at['step', 'no step']\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = (TP) / (TP + FP)\n",
    "    recall = (TP) / (TP + FN)\n",
    "    f_score = (precision * recall) / (precision + recall)\n",
    "    total_accuracy.append(accuracy)\n",
    "    total_precision.append(precision)\n",
    "    total_recall.append(recall) \n",
    "    total_f_score.append(f_score) \n",
    "    \n",
    "print(f\"Accuracy in %:  {np.round(np.mean(total_accuracy)*100, 2)}\")\n",
    "print(f\"Precision in %: {np.round(np.mean(total_precision)*100, 2)}\")\n",
    "print(f\"Recall in %:    {np.round(np.mean(total_recall)*100, 2)}\")\n",
    "print(f\"F-Score in %:   {np.round(np.mean(total_f_score)*100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Footsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_foot_arrows(foot_data, ax, left_color, right_color):\n",
    "    for f, t, color in [('LFoot', 'LToe', left_color), ('RFoot', 'RToe', right_color)]:\n",
    "        for (i, foot), (_, toe) in zip(foot_data['pos'][f].iterrows(), foot_data['pos'][t].iterrows()):\n",
    "            x = foot['X']\n",
    "            y = foot['Z']\n",
    "            d_x = toe['X'] - x\n",
    "            d_y = toe['Z'] - y\n",
    "            ax.arrow(x, y, d_x, d_y, color=color, head_width=3)\n",
    "        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# fig.suptitle('Einige erkannte Schritte der Grundwahrheit')\n",
    "fig.suptitle('Modell 6b)')\n",
    "ax.set(xlabel='X in cm', ylabel='Y in cm')\n",
    "\n",
    "ax.set_aspect('equal', 'box')\n",
    "\n",
    "draw_foot_arrows(gt_foots,         ax, 'lightsteelblue', 'bisque')\n",
    "draw_foot_arrows(prediction_foots, ax, 'tab:blue', 'tab:orange')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animate Footsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_foot_arrows(num, foot_data, ax, left_color, right_color):\n",
    "    num -= frames_to_show\n",
    "    for f, t, color in [('LFoot', 'LToe', left_color), ('RFoot', 'RToe', right_color)]:\n",
    "        foot_iter = foot_data.pos[f].loc[foot_data.pos[f].index.isin(range(num, num+frames_to_show))].iterrows()\n",
    "        toe_iter  = foot_data.pos[t].loc[foot_data.pos[t].index.isin(range(num, num+frames_to_show))].iterrows()\n",
    "        for (i, foot), (_, toe) in zip(foot_iter, toe_iter):\n",
    "            x = foot['X']\n",
    "            y = foot['Z']\n",
    "            d_x = toe['X'] - x\n",
    "            d_y = toe['Z'] - y\n",
    "            ax.arrow(x, y, d_x, d_y, color=color, head_width=3)\n",
    "        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# fig.suptitle('Einige erkannte Schritte der Grundwahrheit')\n",
    "# fig.suptitle('Modell 6b)')\n",
    "ax.set(xlabel='X in cm', ylabel='Y in cm')\n",
    "\n",
    "ax.set_xlim(-250,250)\n",
    "ax.set_ylim(-250,250)\n",
    "\n",
    "legend_elements = [Line2D([0], [0], color='tab:orange', lw=4, label='Links Grundw.'),\n",
    "                   Line2D([0], [0], color='tab:blue', lw=4, label='Links Model'),\n",
    "                   Line2D([0], [0], color='bisque', lw=4, label='Rechts Grundw.'),\n",
    "                   Line2D([0], [0], color='lightsteelblue', lw=4, label='Rechts Model'),\n",
    "                   ]\n",
    "\n",
    "def animate(num):  \n",
    "    ax.clear()\n",
    "    ax.set_xlim(-250,250)\n",
    "    ax.set_ylim(-250,250)\n",
    "    ax.set(xlabel='X in cm', ylabel='Y in cm')\n",
    "    ax.legend(handles=legend_elements, loc='lower left')\n",
    "    draw_foot_arrows(num, gt_foots,         ax, 'tab:orange', 'bisque')\n",
    "    draw_foot_arrows(num, prediction_foots, ax, 'tab:blue', 'lightsteelblue')\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=length, interval=1000/FPS, blit=False)\n",
    "\n",
    "f = f\"./foot_position_graph_{start_frame}-{start_frame+length}_{FPS}fps.mp4\" \n",
    "writervideo = animation.FFMpegWriter(fps=FPS) \n",
    "anim.save(f, writer=writervideo)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = [[ 66386, 174296,  75131, 577908,  32015],\n",
    "        [ 58230, 381139,  78045,  99308, 160454],\n",
    "        [ 89135,  80552, 152558, 497981, 603535],\n",
    "        [ 78415,  81858, 150656, 193263,  69638],\n",
    "        [139361, 331509, 343164, 781380,  52269]]\n",
    "\n",
    "columns = ('Freeze', 'Wind', 'Flood', 'Quake', 'Hail')\n",
    "rows = ['%d year' % x for x in (100, 50, 20, 10, 5)]\n",
    "\n",
    "values = np.arange(0, 2500, 500)\n",
    "value_increment = 1000\n",
    "\n",
    "# Get some pastel shades for the colors\n",
    "colors = plt.cm.BuPu(np.linspace(0, 0.5, len(rows)))\n",
    "n_rows = len(data)\n",
    "\n",
    "index = np.arange(len(columns)) + 0.3\n",
    "bar_width = 0.4\n",
    "\n",
    "# Initialize the vertical-offset for the stacked bar chart.\n",
    "y_offset = np.zeros(len(columns))\n",
    "\n",
    "# Plot bars and create text labels for the table\n",
    "cell_text = []\n",
    "for row in range(n_rows):\n",
    "    plt.bar(index, data[row], bar_width, bottom=y_offset, color=colors[row])\n",
    "    y_offset = y_offset + data[row]\n",
    "    cell_text.append(['%1.1f' % (x / 1000.0) for x in y_offset])\n",
    "# Reverse colors and text labels to display the last value at the top.\n",
    "colors = colors[::-1]\n",
    "cell_text.reverse()\n",
    "\n",
    "# Add a table at the bottom of the axes\n",
    "the_table = plt.table(cellText=cell_text,\n",
    "                      rowLabels=rows,\n",
    "                      rowColours=colors,\n",
    "                      colLabels=columns,\n",
    "                      loc='center')\n",
    "\n",
    "# Adjust layout to make room for the table:\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2)\n",
    "\n",
    "plt.ylabel(\"Loss in ${0}'s\".format(value_increment))\n",
    "plt.yticks(values * value_increment, ['%d' % val for val in values])\n",
    "plt.xticks([])\n",
    "plt.title('Loss by Disaster')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def set_text_row(table, text_array, column_index=0):\n",
    "    if text_array.shape == (): text_array = np.expand_dims(text_array, 0)\n",
    "    text_array = text_array[::-1]\n",
    "    for i in range(min(text_array.size, 5)):\n",
    "        table.get_celld()[(i+1, column_index)].get_text().set_text(text_array[i]) \n",
    "\n",
    "left_foot_gt_strides = pd.DataFrame(\n",
    "    np.concatenate((np.array([0]), gt_foots.stride_len.LFoot)), \n",
    "    index=gt_foots.i.LFoot, \n",
    "    columns=[\"stride_len\"]\n",
    ")\n",
    "\n",
    "left_foot_pred_strides = pd.DataFrame(\n",
    "    np.concatenate((np.array([0]), prediction_foots.stride_len.LFoot)), \n",
    "    index=prediction_foots.i.LFoot, \n",
    "    columns=[\"stride_len\"]\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "columns = ('Grundw.', 'Modell')\n",
    "\n",
    "# cell_colours =np.array(['tab:orange', 'tab:blue'])\n",
    "\n",
    "table = ax.table(cellColours=np.array(['white']).repeat(10).reshape(5,2),\n",
    "                 colColours=['tab:orange', 'tab:blue'],\n",
    "                 colLabels=columns,\n",
    "                 loc='center')\n",
    "for (row, col), cell in table.get_celld().items():\n",
    "    if (row == 0):\n",
    "        cell.set_text_props(fontproperties=FontProperties(weight='bold'))\n",
    "    if (row > 1):\n",
    "        cell.get_text().set_color('lightgrey')\n",
    "            \n",
    "table.set_fontsize(34)\n",
    "table.scale(1, 4)\n",
    "\n",
    "\n",
    "def animate(num):\n",
    "    set_text_row(table, left_foot_gt_strides.loc[num-frames_to_show:num].to_numpy().squeeze().round(2), 0)\n",
    "    set_text_row(table, left_foot_pred_strides.loc[num-frames_to_show:num].to_numpy().squeeze().round(2), 1)\n",
    "\n",
    "\n",
    "# for num in range(0, length):\n",
    "#     set_text_row(table, left_foot_gt_strides.loc[num-frames_to_show:num].to_numpy().squeeze().round(2), 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=length, interval=1000/FPS, blit=False)\n",
    "\n",
    "f = f\"./stride_length_table_{start_frame}-{start_frame+length}_{FPS}fps.mp4\" \n",
    "writervideo = animation.FFMpegWriter(fps=FPS) \n",
    "anim.save(f, writer=writervideo)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate optimal distance for Stride length Calculation\n",
    "Depending on representation (either bones/rotation or joints) the stride length calculation calculates different steps for the ground truth. This script iterates over the distance parameter to get the value for which the calculated ground truth steps are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "max_distance = 100\n",
    "for distance in range(1, max_distance):\n",
    "    foot_pos, _ = calc_standing_foot_positions(test_data.optitrack_data, distance=distance)\n",
    "    distances.append(foot_pos['stride_len'].apply(len).mean())\n",
    "    print(f\"\\rProgress: {distance/max_distance*100:.1f}%   \", end='')\n",
    "\n",
    "print(\"\\rProgress: 100.0%   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, max_distance)), distances)\n",
    "plt.title(\"Erkannte Schritte über Mindestabstand\")\n",
    "plt.xlabel(\"Mindestabstand\")\n",
    "plt.ylabel(\"Erkannte Schritte\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
